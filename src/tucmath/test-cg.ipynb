{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import scipy as sp\n",
    "from scipy.sparse.linalg import cg\n",
    "from sklearn.metrics.pairwise import rbf_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: susy\n",
      "--------\n",
      "Shape train data: (5000, 18)\n",
      "Shape test data: (5000, 18)\n"
     ]
    }
   ],
   "source": [
    "from data_datasets import higgs, susy, cod_rna\n",
    "N =5000\n",
    "data = \"susy\"\n",
    "# data = \"higgs\"\n",
    "# data = \"cod_rna\"\n",
    "####################\n",
    "\n",
    "if data == \"higgs\":\n",
    "    X_train, X_test, y_train, y_test = higgs(N)\n",
    "elif data == \"susy\":\n",
    "    X_train, X_test, y_train, y_test = susy(N)\n",
    "elif data == \"cod_rna\":\n",
    "    X_train, X_test, y_train, y_test = cod_rna(N)\n",
    "\n",
    "print(\"\\nDataset:\", data)\n",
    "print(\"--------\\nShape train data:\", X_train.shape)\n",
    "print(\"Shape test data:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(float).eps\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nystrom(K, rank):\n",
    "    N = K.shape[0]\n",
    "    Omega = np.random.randn(N, rank)\n",
    "    Y = K@Omega  # matvec\n",
    "    eps = np.finfo(float).eps\n",
    "    shift = np.linalg.norm(Y, \"fro\")*eps\n",
    "    Y_shift = Y+shift*Omega\n",
    "    C = np.linalg.cholesky(Omega.T@Y_shift)\n",
    "    B = Y_shift @ np.linalg.inv(C)\n",
    "\n",
    "    U, D, VT = np.linalg.svd(B, full_matrices=False)\n",
    "    Sigma = np.diag(D)\n",
    "    Lamda = np.maximum(0, Sigma@Sigma-shift*np.eye(rank))\n",
    "\n",
    "    return U, Lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from low_rank_methods import nystrom\n",
    "mu = .1\n",
    "rank = 20\n",
    "\n",
    "N, d = X_train.shape\n",
    "I = np.eye(N)\n",
    "i = np.eye(rank)\n",
    "\n",
    "K = rbf_kernel(X_train, gamma=.1)\n",
    "K_reg=K+mu*I\n",
    "U = nystrom(K, rank)[0]\n",
    "D = nystrom(K, rank)[1]\n",
    "print(U.shape, D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import diag_inv\n",
    "\n",
    "residuals_pre = []\n",
    "b = y_train\n",
    "B = diag_inv(D+mu)\n",
    "lambda_l = D[-1, -1]\n",
    "\n",
    "P_nys = U@(D+mu)@U.T/(lambda_l+mu)+I-U@U.T\n",
    "P_nys_inv = (lambda_l+mu)*U@B@U.T+I-U@U.T\n",
    "\n",
    "\n",
    "def callback(x): return residuals_pre.append(\n",
    "    np.linalg.norm(K_reg @ x - b) )\n",
    "# / np.linalg.norm(b)\n",
    "\n",
    "alpha_pre, info = cg(K_reg, b, M=P_nys_inv, callback=callback, tol=1e-3)\n",
    "len(residuals_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = []\n",
    "b = y_train\n",
    "\n",
    "\n",
    "def callback(x): return residuals.append(\n",
    "    np.linalg.norm(K_reg @ x - b) )\n",
    "# / np.linalg.norm(b)\n",
    "\n",
    "\n",
    "alpha, info = cg(K_reg, b, callback=callback, tol=1e-3)\n",
    "len(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import scipy.linalg as la\n",
    "\n",
    "\n",
    "\n",
    "def CG(A, b, P_inv=None, matvec=None, tol=1e-5, x0=None, max_iter=None):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient Method for solving Ax = b\n",
    "    :param A: matrix\n",
    "    :param b: vector\n",
    "    :param tol: tolerance (default: 1e-5)\n",
    "    :param x0: initial guess (default: Zero vector)\n",
    "    :param max_iter: Maximum number of iterations (default: Dimention of A)\n",
    "    :param P_inv:  The inveresidualse of Preconditioner (default: None for CG)\n",
    "    :return: x, residuals\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(b)\n",
    "    residuals = []\n",
    "    dot = np.dot\n",
    "\n",
    "    if max_iter is None:\n",
    "        max_iter = N\n",
    "\n",
    "    def matvec(M, u):\n",
    "        return M@u\n",
    "\n",
    "    x = np.zeros(N) if x0 == None else x0\n",
    "    r = b - matvec(A, x) if x.any() else b.copy()\n",
    "    p = r\n",
    "    z = np.zeros(N)\n",
    "\n",
    "    if (P_inv is None):\n",
    "        method = \"cg\"\n",
    "    else:\n",
    "        method = \"pcg\"\n",
    "        z = P_inv@r\n",
    "\n",
    "    # Main Loop\n",
    "    i = 0\n",
    "    while i < max_iter or la.norm(r) > tol:\n",
    "        residuals.append(np.linalg.norm(r))\n",
    "\n",
    "        if la.norm(r) < tol:\n",
    "            return x, np.array(residuals)\n",
    "\n",
    "        v = matvec(A, p)\n",
    "        v = np.nan_to_num(v)\n",
    "\n",
    "        if method == \"cg\":\n",
    "            alpha = dot(r, r) / dot(p, v)\n",
    "            alpha = np.nan_to_num(alpha)\n",
    "\n",
    "\n",
    "        elif method == \"pcg\":\n",
    "            alpha = dot(r, z) / dot(p, v)\n",
    "            alpha = np.nan_to_num(alpha)\n",
    "\n",
    "\n",
    "        # Update x and r\n",
    "        x = x + alpha * p\n",
    "        x = np.nan_to_num(x)\n",
    "\n",
    "        r_new = r - alpha * v\n",
    "        r_new = np.nan_to_num(r_new)\n",
    "\n",
    "\n",
    "        # Update x and r\n",
    "\n",
    "        if method == \"cg\":\n",
    "            beta = dot(r_new, r_new)/dot(r, r)\n",
    "            beta = np.nan_to_num(beta)\n",
    "\n",
    "        elif method == \"pcg\":\n",
    "            z_new = P_inv@r_new\n",
    "            beta = dot(z_new, r_new)/dot(z, r)\n",
    "            beta = np.nan_to_num(beta)\n",
    "\n",
    "\n",
    "        p = r_new + beta * p\n",
    "        p = np.nan_to_num(p)\n",
    "\n",
    "        r = r_new\n",
    "        i = i+1\n",
    "\n",
    "    return x, np.array(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Show plot with convergence profile - normalised residual vector vs iteration.\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Pre KRR method convergence profile.')\n",
    "plt.ylabel('Convergence (residual norm)')\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "m_res = residuals\n",
    "plt.semilogy(range(len(m_res)), m_res, 'b--')\n",
    "\n",
    "plt.legend(['Total iter = ' + str(len(m_res))])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
